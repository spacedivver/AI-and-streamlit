# AI ê¸°ë°˜ ì½”ë”” ì¶”ì²œ ì„œë¹„ìŠ¤

import streamlit as st
from PIL import Image
from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate

# OpenAI API í‚¤ ì„¤ì •
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]

# GPT-4 ëª¨ë¸ ì´ˆê¸°í™”
llm = ChatOpenAI(
    model_name="gpt-4",
    temperature=0.7,
    openai_api_key=OPENAI_API_KEY
)

# CLIP ëª¨ë¸ ì´ˆê¸°í™”
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# BLIP ëª¨ë¸ ì´ˆê¸°í™” (ì´ë¯¸ì§€ ìº¡ì…”ë‹)
blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")

# Streamlit ì œëª©
st.title("ğŸ‘— AI ê¸°ë°˜ ì½”ë”” ì¶”ì²œ ì„œë¹„ìŠ¤")

# ì´ë¯¸ì§€ ì—…ë¡œë“œ ì„¹ì…˜
st.sidebar.header('ì´ë¯¸ì§€ ì—…ë¡œë“œ')
uploaded_file = st.sidebar.file_uploader("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (ì˜·ì„ ì…ì€ ì‚¬ëŒì˜ ì‚¬ì§„)", type=["jpg", "jpeg", "png"])


if uploaded_file:
    # ì´ë¯¸ì§€ ë¡œë“œ ë° í‘œì‹œ
    image = Image.open(uploaded_file)
    st.image(image, caption="ì—…ë¡œë“œëœ ì´ë¯¸ì§€", use_column_width=True)

    # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì…ë ¥
    user_prompt = st.text_input("í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 'ìºì£¼ì–¼í•œ ëª¨ì„ì— ì–´ìš¸ë¦¬ëŠ” ì½”ë””')")

    if user_prompt and st.button("ë¶„ì„ ì‹œì‘"):
        with st.spinner("ì´ë¯¸ì§€ ë° í”„ë¡¬í”„íŠ¸ ë¶„ì„ ì¤‘..."):
            # BLIP ëª¨ë¸ë¡œ ì´ë¯¸ì§€ ìº¡ì…”ë‹
            inputs = blip_processor(image, return_tensors="pt")
            out = blip_model.generate(**inputs)
            caption = blip_processor.decode(out[0], skip_special_tokens=True)

            # CLIP ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ë¶„ì„ (ì¶”ê°€ì ì¸ ë¶„ì„ ê°€ëŠ¥)
            inputs_clip = clip_processor(text=[user_prompt], images=image, return_tensors="pt", padding=True)
            outputs_clip = clip_model(**inputs_clip)
            logits_per_image = outputs_clip.logits_per_image  # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ ì ìˆ˜

            # GPT-4ë¥¼ ì‚¬ìš©í•˜ì—¬ ì½”ë”” ì¶”ì²œ ìƒì„±
            prompt_template = PromptTemplate(
                input_variables=["image_caption", "user_prompt"],
                template="""
                ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸: {user_prompt}

                ì—…ë¡œë“œëœ ì´ë¯¸ì§€ ì„¤ëª…: {image_caption}

                ì‚¬ìš©ìê°€ ìš”ì²­í•œ ìƒí™©ì— ë§ê²Œ ì½”ë””ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”. 
                - ìƒì˜, í•˜ì˜, ì‹ ë°œ ë“± ë³€ê²½ì´ í•„ìš”í•œ í•­ëª©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ê³  ì¶”ì²œí•´ì£¼ì„¸ìš”.
                """
            )

            # GPT-4ë¡œ ì½”ë”” ì¶”ì²œ ìƒì„±
            reasoning_input = {
                "image_caption": caption,
                "user_prompt": user_prompt,
            }
            recommendation = llm.predict(prompt_template.format(**reasoning_input))

        # ê²°ê³¼ ì¶œë ¥
        st.success("ë¶„ì„ ì™„ë£Œ!")
        # st.write("## ì´ë¯¸ì§€ ìº¡ì…”ë‹ ê²°ê³¼")
        # st.write(f"**ì´ë¯¸ì§€ ì„¤ëª…:** {caption}")
        st.write("## ì½”ë”” ì¶”ì²œ ê²°ê³¼")
        st.write(recommendation)
